{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 9\n",
    "#### Student Name: Daniel Beckley\n",
    "#### ID: 8846774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import reuters, stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import seed, sample\n",
    "nltk.download('reuters') #downloading reuters corpus\n",
    "len(reuters.fileids()) #checking how many files are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:  # Default case\n",
    "        return None\n",
    "\n",
    "\n",
    "def nlp_preprocess(sample_paragraph):\n",
    "    # Lowercase the text\n",
    "    text = sample_paragraph.lower()\n",
    "    # Replace special HTML entities with their character equivalents\n",
    "    text = re.sub(r'&lt;', '<', text)\n",
    "    text = re.sub(r'&gt;', '>', text)\n",
    "    # Remove periods following numbers\n",
    "    text = re.sub(r'(?<=\\d)\\.', '', text)\n",
    "    # Replace hyphens in compound words with spaces\n",
    "    text = re.sub(r'-', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove tokens that are only punctuation, but allow numbers and words\n",
    "    punct_set = set(string.punctuation)\n",
    "    tokens = [token for token in tokens if (token not in punct_set) and (any(char.isalnum() for char in token))]\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # POS Tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample sentences above. You are required for this assignment to implement four functions **from scratch**. <br>\n",
    "You are required to preprocess the text and apply the tokenization process as done in assignment 8. (3)\n",
    "***THEN***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'versatile', 'program', 'language']\n",
      "['javascript', 'widely', 'use', 'web', 'development']\n",
      "['java', 'know', 'platform', 'independence']\n",
      "['program', 'involves', 'write', 'code', 'solve', 'problem']\n",
      "['data', 'structure', 'crucial', 'efficient', 'programming']\n",
      "['algorithms', 'step', 'step', 'instruction', 'solve', 'problem']\n",
      "['version', 'control', 'system', 'help', 'manage', 'code', 'change', 'collaboration']\n",
      "['debug', 'process', 'find', 'fix', 'error', 'code']\n",
      "['web', 'framework', 'simplify', 'development', 'web', 'application']\n",
      "['artificial', 'intelligence', 'apply', 'various', 'programming', 'task']\n",
      "[['python', 'versatile', 'program', 'language'], ['javascript', 'widely', 'use', 'web', 'development'], ['java', 'know', 'platform', 'independence'], ['program', 'involves', 'write', 'code', 'solve', 'problem'], ['data', 'structure', 'crucial', 'efficient', 'programming'], ['algorithms', 'step', 'step', 'instruction', 'solve', 'problem'], ['version', 'control', 'system', 'help', 'manage', 'code', 'change', 'collaboration'], ['debug', 'process', 'find', 'fix', 'error', 'code'], ['web', 'framework', 'simplify', 'development', 'web', 'application'], ['artificial', 'intelligence', 'apply', 'various', 'programming', 'task']]\n"
     ]
    }
   ],
   "source": [
    "## TODO: Clean the sentences and print the lemmatized tokens\n",
    "for sentence in sample_sentences:\n",
    "    print(nlp_preprocess(sentence))\n",
    "    \n",
    "tokenized_sentences = [nlp_preprocess(sentence) for sentence in sample_sentences]\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>python</td>\n",
       "      <td>versatile</td>\n",
       "      <td>program</td>\n",
       "      <td>language</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>javascript</td>\n",
       "      <td>widely</td>\n",
       "      <td>use</td>\n",
       "      <td>web</td>\n",
       "      <td>development</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java</td>\n",
       "      <td>know</td>\n",
       "      <td>platform</td>\n",
       "      <td>independence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>program</td>\n",
       "      <td>involves</td>\n",
       "      <td>write</td>\n",
       "      <td>code</td>\n",
       "      <td>solve</td>\n",
       "      <td>problem</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data</td>\n",
       "      <td>structure</td>\n",
       "      <td>crucial</td>\n",
       "      <td>efficient</td>\n",
       "      <td>programming</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>step</td>\n",
       "      <td>step</td>\n",
       "      <td>instruction</td>\n",
       "      <td>solve</td>\n",
       "      <td>problem</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>version</td>\n",
       "      <td>control</td>\n",
       "      <td>system</td>\n",
       "      <td>help</td>\n",
       "      <td>manage</td>\n",
       "      <td>code</td>\n",
       "      <td>change</td>\n",
       "      <td>collaboration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>debug</td>\n",
       "      <td>process</td>\n",
       "      <td>find</td>\n",
       "      <td>fix</td>\n",
       "      <td>error</td>\n",
       "      <td>code</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>web</td>\n",
       "      <td>framework</td>\n",
       "      <td>simplify</td>\n",
       "      <td>development</td>\n",
       "      <td>web</td>\n",
       "      <td>application</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>artificial</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>apply</td>\n",
       "      <td>various</td>\n",
       "      <td>programming</td>\n",
       "      <td>task</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1         2             3            4   \n",
       "1       python     versatile   program      language         None  \\\n",
       "2   javascript        widely       use           web  development   \n",
       "3         java          know  platform  independence         None   \n",
       "4      program      involves     write          code        solve   \n",
       "5         data     structure   crucial     efficient  programming   \n",
       "6   algorithms          step      step   instruction        solve   \n",
       "7      version       control    system          help       manage   \n",
       "8        debug       process      find           fix        error   \n",
       "9          web     framework  simplify   development          web   \n",
       "10  artificial  intelligence     apply       various  programming   \n",
       "\n",
       "              5       6              7  \n",
       "1          None    None           None  \n",
       "2          None    None           None  \n",
       "3          None    None           None  \n",
       "4       problem    None           None  \n",
       "5          None    None           None  \n",
       "6       problem    None           None  \n",
       "7          code  change  collaboration  \n",
       "8          code    None           None  \n",
       "9   application    None           None  \n",
       "10         task    None           None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tokenized_sentences)\n",
    "df.index = df.index + 1\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the inverted index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    # Step 1: Generate (Token, DocumentID) pairs\n",
    "    token_doc_pairs = []\n",
    "    for i, tokens in enumerate(list_of_sentence_tokens):\n",
    "        doc_id = i + 1 \n",
    "        for token in tokens:\n",
    "            token_doc_pairs.append((token, doc_id))\n",
    "\n",
    "    # Step 2: Sort the token-doc pairs by token\n",
    "    sorted_token_doc_pairs = sorted(token_doc_pairs, key=lambda x: x[0])\n",
    "\n",
    "    # Step 3: Build the inverted index from the sorted list\n",
    "    inverted_index = {}\n",
    "    for token, doc_id in sorted_token_doc_pairs:\n",
    "        if token not in inverted_index:\n",
    "            inverted_index[token] = [doc_id]\n",
    "        elif doc_id not in inverted_index[token]:\n",
    "            inverted_index[token].append(doc_id)\n",
    "\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithms': [6],\n",
       " 'application': [9],\n",
       " 'apply': [10],\n",
       " 'artificial': [10],\n",
       " 'change': [7],\n",
       " 'code': [4, 7, 8],\n",
       " 'collaboration': [7],\n",
       " 'control': [7],\n",
       " 'crucial': [5],\n",
       " 'data': [5],\n",
       " 'debug': [8],\n",
       " 'development': [2, 9],\n",
       " 'efficient': [5],\n",
       " 'error': [8],\n",
       " 'find': [8],\n",
       " 'fix': [8],\n",
       " 'framework': [9],\n",
       " 'help': [7],\n",
       " 'independence': [3],\n",
       " 'instruction': [6],\n",
       " 'intelligence': [10],\n",
       " 'involves': [4],\n",
       " 'java': [3],\n",
       " 'javascript': [2],\n",
       " 'know': [3],\n",
       " 'language': [1],\n",
       " 'manage': [7],\n",
       " 'platform': [3],\n",
       " 'problem': [4, 6],\n",
       " 'process': [8],\n",
       " 'program': [1, 4],\n",
       " 'programming': [5, 10],\n",
       " 'python': [1],\n",
       " 'simplify': [9],\n",
       " 'solve': [4, 6],\n",
       " 'step': [6],\n",
       " 'structure': [5],\n",
       " 'system': [7],\n",
       " 'task': [10],\n",
       " 'use': [2],\n",
       " 'various': [10],\n",
       " 'versatile': [1],\n",
       " 'version': [7],\n",
       " 'web': [2, 9],\n",
       " 'widely': [2],\n",
       " 'write': [4]}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_inverted_index(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the Positional index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "    # Step 1: Generate (Token, DocumentID, Position) tuples\n",
    "    token_doc_pos_tuples = []\n",
    "    for i, tokens in enumerate(list_of_sentence_tokens):\n",
    "        doc_id = i + 1 \n",
    "        for position, token in enumerate(tokens):\n",
    "            token_doc_pos_tuples.append((token, doc_id, position))\n",
    "\n",
    "    # Step 2: Sort the tuples by token (and then by document ID for consistency)\n",
    "    sorted_token_doc_pos_tuples = sorted(token_doc_pos_tuples, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    # Step 3: Build the positional index from the sorted list\n",
    "    positional_index = {}\n",
    "    for token, doc_id, position in sorted_token_doc_pos_tuples:\n",
    "        if token not in positional_index:\n",
    "            positional_index[token] = {doc_id: [position]}\n",
    "        elif doc_id not in positional_index[token]:\n",
    "            positional_index[token][doc_id] = [position]\n",
    "        else:\n",
    "            positional_index[token][doc_id].append(position)\n",
    "\n",
    "    return positional_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithms': {6: [0]},\n",
       " 'application': {9: [5]},\n",
       " 'apply': {10: [2]},\n",
       " 'artificial': {10: [0]},\n",
       " 'change': {7: [6]},\n",
       " 'code': {4: [3], 7: [5], 8: [5]},\n",
       " 'collaboration': {7: [7]},\n",
       " 'control': {7: [1]},\n",
       " 'crucial': {5: [2]},\n",
       " 'data': {5: [0]},\n",
       " 'debug': {8: [0]},\n",
       " 'development': {2: [4], 9: [3]},\n",
       " 'efficient': {5: [3]},\n",
       " 'error': {8: [4]},\n",
       " 'find': {8: [2]},\n",
       " 'fix': {8: [3]},\n",
       " 'framework': {9: [1]},\n",
       " 'help': {7: [3]},\n",
       " 'independence': {3: [3]},\n",
       " 'instruction': {6: [3]},\n",
       " 'intelligence': {10: [1]},\n",
       " 'involves': {4: [1]},\n",
       " 'java': {3: [0]},\n",
       " 'javascript': {2: [0]},\n",
       " 'know': {3: [1]},\n",
       " 'language': {1: [3]},\n",
       " 'manage': {7: [4]},\n",
       " 'platform': {3: [2]},\n",
       " 'problem': {4: [5], 6: [5]},\n",
       " 'process': {8: [1]},\n",
       " 'program': {1: [2], 4: [0]},\n",
       " 'programming': {5: [4], 10: [4]},\n",
       " 'python': {1: [0]},\n",
       " 'simplify': {9: [2]},\n",
       " 'solve': {4: [4], 6: [4]},\n",
       " 'step': {6: [1, 2]},\n",
       " 'structure': {5: [1]},\n",
       " 'system': {7: [2]},\n",
       " 'task': {10: [5]},\n",
       " 'use': {2: [2]},\n",
       " 'various': {10: [3]},\n",
       " 'versatile': {1: [1]},\n",
       " 'version': {7: [0]},\n",
       " 'web': {2: [3], 9: [0, 4]},\n",
       " 'widely': {2: [1]},\n",
       " 'write': {4: [2]}}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_positional_index(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the TF-IDF Matrix that is sufficient to represent the documents. Assume that each sentence is a document and the sentence ID starts from 1. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the TF - IDF matrix by following the below steps:\n",
    "\n",
    "Calculate Term Frequency (TF): The number of times a word appears in a sentence divided by the total number of words in that sentence.\n",
    "\n",
    "Calculate Document Frequency (DF): The number of documents containing a word.\n",
    "\n",
    "Calculate Inverse Document Frequency (IDF): The log of the total number of sentences divided by the document frequency of each word.\n",
    "\n",
    "Compute TF-IDF: Multiply TF by IDF for each word in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TF(word_dict, tokenized_sentence):\n",
    "    tf_dict = {}\n",
    "    sentence_length = len(tokenized_sentence)\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count / float(sentence_length)\n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IDF(doc_list):\n",
    "    idf_dict = {}\n",
    "    N = len(doc_list)\n",
    "    \n",
    "    idf_dict = dict.fromkeys(doc_list[0].keys(), 0)\n",
    "    for doc in doc_list:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idf_dict[word] += 1\n",
    "    \n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / float(val))\n",
    "        \n",
    "    return idf_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TFIDF(tf_dict, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tf_dict.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the tf-idf matrix\n",
    "    word_set = set(word for sentence in list_of_sentence_tokens for word in sentence)\n",
    "    sorted_word_set = sorted(word_set)  \n",
    "    # display(sorted_word_set)\n",
    "    \n",
    "    # Initialize document list\n",
    "    doc_list = []\n",
    "    for sentence in list_of_sentence_tokens:\n",
    "        word_dict = dict.fromkeys(sorted_word_set, 0)  # Use sorted_word_set\n",
    "        for word in sentence:\n",
    "            word_dict[word] += 1\n",
    "        doc_list.append(word_dict)\n",
    "    # display(pd.DataFrame(doc_list))\n",
    "    \n",
    "    # Compute TF for each sentence\n",
    "    tf_list = [compute_TF(doc, sentence) for doc, sentence in zip(doc_list, list_of_sentence_tokens)]\n",
    "    # display(tf_list)\n",
    "    # Compute IDF\n",
    "    idfs = compute_IDF(doc_list)\n",
    "    # display(pd.DataFrame([idfs]))\n",
    "    \n",
    "    # Compute TF-IDF for each sentence\n",
    "    tfidf_list = [compute_TFIDF(tf, idfs) for tf in tf_list]\n",
    "    \n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basic</th>\n",
       "      <th>commands</th>\n",
       "      <th>data</th>\n",
       "      <th>dvc</th>\n",
       "      <th>essential</th>\n",
       "      <th>for</th>\n",
       "      <th>linux</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      basic  commands  data       dvc  essential  for     linux  science\n",
       "0  0.115525       0.0   0.0  0.000000   0.000000  0.0  0.115525      0.0\n",
       "1  0.000000       0.0   0.0  0.115525   0.115525  0.0  0.000000      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ss = [['basic', 'linux', 'commands', 'for', 'data', 'science'], ['essential', 'dvc', 'commands', 'for', 'data', 'science']]\n",
    "display(pd.DataFrame(get_TFIDF_matrix(ss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jupiter</td>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "      <td>largest</td>\n",
       "      <td>planet</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mars</td>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "      <td>fourth</td>\n",
       "      <td>planet</td>\n",
       "      <td>from</td>\n",
       "      <td>the</td>\n",
       "      <td>sun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1    2        3       4     5     6     7\n",
       "0  jupiter  is  the  largest  planet  None  None  None\n",
       "1     mars  is  the   fourth  planet  from   the   sun"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fourth</th>\n",
       "      <th>from</th>\n",
       "      <th>is</th>\n",
       "      <th>jupiter</th>\n",
       "      <th>largest</th>\n",
       "      <th>mars</th>\n",
       "      <th>planet</th>\n",
       "      <th>sun</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fourth      from   is   jupiter   largest      mars  planet       sun   \n",
       "0  0.000000  0.000000  0.0  0.138629  0.138629  0.000000     0.0  0.000000  \\\n",
       "1  0.086643  0.086643  0.0  0.000000  0.000000  0.086643     0.0  0.086643   \n",
       "\n",
       "   the  \n",
       "0  0.0  \n",
       "1  0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ff = [['jupiter', 'is', 'the', 'largest', 'planet'], ['mars', 'is', 'the', 'fourth', 'planet', 'from', 'the', 'sun']]\n",
    "display(pd.DataFrame(ff))\n",
    "display(pd.DataFrame(get_TFIDF_matrix(ff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithms</th>\n",
       "      <th>application</th>\n",
       "      <th>apply</th>\n",
       "      <th>artificial</th>\n",
       "      <th>change</th>\n",
       "      <th>code</th>\n",
       "      <th>collaboration</th>\n",
       "      <th>control</th>\n",
       "      <th>crucial</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>structure</th>\n",
       "      <th>system</th>\n",
       "      <th>task</th>\n",
       "      <th>use</th>\n",
       "      <th>various</th>\n",
       "      <th>versatile</th>\n",
       "      <th>version</th>\n",
       "      <th>web</th>\n",
       "      <th>widely</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.460517</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460517</td>\n",
       "      <td>0.460517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.383764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287823</td>\n",
       "      <td>0.150497</td>\n",
       "      <td>0.287823</td>\n",
       "      <td>0.287823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383764</td>\n",
       "      <td>0.383764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   algorithms  application     apply  artificial    change      code   \n",
       "0    0.000000     0.000000  0.000000    0.000000  0.000000  0.000000  \\\n",
       "1    0.000000     0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "2    0.000000     0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "3    0.000000     0.000000  0.000000    0.000000  0.000000  0.200662   \n",
       "4    0.000000     0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "5    0.383764     0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "6    0.000000     0.000000  0.000000    0.000000  0.287823  0.150497   \n",
       "7    0.000000     0.000000  0.000000    0.000000  0.000000  0.200662   \n",
       "8    0.000000     0.383764  0.000000    0.000000  0.000000  0.000000   \n",
       "9    0.000000     0.000000  0.383764    0.383764  0.000000  0.000000   \n",
       "\n",
       "   collaboration   control   crucial      data  ...  structure    system   \n",
       "0       0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000  \\\n",
       "1       0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "3       0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "4       0.000000  0.000000  0.460517  0.460517  ...   0.460517  0.000000   \n",
       "5       0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "6       0.287823  0.287823  0.000000  0.000000  ...   0.000000  0.287823   \n",
       "7       0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "8       0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "9       0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "\n",
       "       task       use   various  versatile   version       web    widely   \n",
       "0  0.000000  0.000000  0.000000   0.575646  0.000000  0.000000  0.000000  \\\n",
       "1  0.000000  0.460517  0.000000   0.000000  0.000000  0.321888  0.460517   \n",
       "2  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000   0.000000  0.287823  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.000000  0.000000   0.000000  0.000000  0.536479  0.000000   \n",
       "9  0.383764  0.000000  0.383764   0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      write  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.000000  \n",
       "3  0.383764  \n",
       "4  0.000000  \n",
       "5  0.000000  \n",
       "6  0.000000  \n",
       "7  0.000000  \n",
       "8  0.000000  \n",
       "9  0.000000  \n",
       "\n",
       "[10 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_matrix = get_TFIDF_matrix(tokenized_sentences)\n",
    "display(pd.DataFrame(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 4: Create a method that takes as an input: (10)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # Tokenize the search query same way as the sentences\n",
    "    # query_tokens = nlp_preprocess(search_query)\n",
    "    # \"algorithms\" isn't matching with \"algorithm\" in the sentences when using nlp_preprocess\n",
    "    \n",
    "    # so we use this instead\n",
    "    query_tokens = search_query.lower().split()\n",
    "    \n",
    "    if method_name == \"inverted\":\n",
    "        # Create an inverted index\n",
    "        inverted_index = get_inverted_index(list_of_sentence_tokens)\n",
    "        \n",
    "        # Document scores based on the number of query tokens they contain\n",
    "        doc_scores = {}\n",
    "        for token in query_tokens:\n",
    "            if token in inverted_index:\n",
    "                for doc_id in inverted_index[token]:\n",
    "                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1\n",
    "        \n",
    "        # Sort documents by their scores in descending order\n",
    "        rank_list = sorted(doc_scores, key=doc_scores.get, reverse=True)\n",
    "    \n",
    "    elif method_name == \"tfidf\":\n",
    "        # Calculate TF-IDF matrix for the list of sentences\n",
    "        tfidf_list = get_TFIDF_matrix(list_of_sentence_tokens)\n",
    "        \n",
    "        # Sum up TF-IDF scores for each document based on the query tokens\n",
    "        doc_scores = {}\n",
    "        for i, tfidf_dict in enumerate(tfidf_list):\n",
    "            doc_id = i + 1\n",
    "            for token in query_tokens:\n",
    "                if token in tfidf_dict:\n",
    "                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + tfidf_dict[token]\n",
    "        \n",
    "        # Sort documents by their TF-IDF scores in descending order\n",
    "        rank_list = sorted(doc_scores, key=doc_scores.get, reverse=True)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Method name must be either 'inverted' or 'tfidf'\")\n",
    "    \n",
    "    # Following the same format that sentence IDs start from 1\n",
    "    rank_list = [doc_id for doc_id in rank_list]\n",
    "    \n",
    "    return rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 9]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ranked_documents(tokenized_sentences, \"inverted\", \"web development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 2, 1, 3, 4, 5, 6, 7, 8, 10]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ranked_documents(tokenized_sentences, \"tfidf\", \"web development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ranked_documents(tokenized_sentences, \"inverted\", \"algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithms', 'step', 'step', 'instruction', 'solve', 'problem']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_preprocess(\"Algorithms are step-by-step instructions for solving problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_preprocess(\"Algorithms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "cscn8010_classic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
